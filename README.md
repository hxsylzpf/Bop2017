# 微软编程之美2017 资格赛总结

## 一、资格赛任务题介绍

​        资格赛任务题是基于文档的问答任务，对于给定的一篇文档和一个从文档中提出的自然语言问题，参赛队伍使用提供的数据集训练模型算法，让模型可以回答问题，回答时仅限于从组成该文档的句子中选出能回答该问题的句子。资格赛开始时公布了训练数据集和开发数据集。

训练集如下格式：

![QQ图片20170718132601](Bop2017\images\QQ图片20170718132601.png)



表格的每一列分别对应答案标签、问题、句子，标签中的1代表正确答案，0代表错误答案。

测试集如下格式：

![QQ图片20170718132721](.\images\QQ图片20170718132721.png)

我们需要给测试集中每个问题的候选答案进行打分，分数越高越可能是正确的答案。

评价标准是MRR  ![QQ图片20170718133603](.\images\QQ图片20170718133603.png)

## 二、资格赛模型

​	我们正好刚刚做了一个基于知识库的问答系统，对于一个问题，首先识别其实体，从知识库中，检索每个实体可能连接的关系，产生所有实体-关系对作为候选答案，对每个候选的实体-关系对计算其产生该问题的概率，以此作为打分。正好很适合这个场景，于是就稍加修改运用到了这个资格赛的问题中，取得了不错的效果。下面介绍一下该模型—==seq2seq+attention+copy==。

### 1、seq2seq模型简介

​	seq2seq模型就是一个序列到序列的模型，由一个序列生成另一个序列，我们可以想到很多应用场景，比如机器翻译、对话系统、问答系统。Seq2seq模型的一端叫做encoder端，顾名思义就是将输入的句子进行编码，通常是通过一系列的矩阵的计算和变换得到一个中间语义向量，我们认为该向量捕获了输入句子的语义信息。利用该信息，指导我们在另一端——称作decoder的一端进行解码，每一时刻生成一个词，通过当前生成的词和语义向量再来指导我们生成下一个词。依次产生yi(0<=i<t)，这样看起来就生成了一个句子。

​	但是单纯的采用中间语义向量来生成一个句子，信息可能是不够的，而且缺乏针对性。比如我当前decoder端要生成的这个词可能跟encoder端的某些词相关性更强，这样应该更偏向于采用那些词的信息来指导当前词的生成。这里就用到了attention机制。参考这篇论文对attention机制的介绍，为了打破总结源端全部信息的限制，attention机制将源端语义信息变成了一个动态改变的向量。一种方式是用当前时刻的隐状态与源端每个时刻的隐状态通过一个多层的神经网络进行匹配，以此得到每个时刻不同的权重，将全1计算的向量变成了有注意力的。在对话或者问答场景中，上句跟下句之间经常有重复的词。在这种情况下，无需对语义进行理解，直接利用copy机制将目标端的某个词直接由源端的某个词复制而来。

参考论文链接：	<http://www.aclweb.org/anthology/P/P16/P16-1154.pdf>

### 2、详细步骤

![QQ图片20170718132601](.\images\QQ图片20170718132601.png)

拿这个问题为例，我们首先利用jieba分词，然后生成词库，并有相对应的预先训练好的wordEmbedding。

对于一个问题和其候选集合，我们首先将问题和候选分词，然后将一个问句变成一个embedding序列，输入到源端的双向RNN网络中去，每个词会产生一个对应时刻的隐状态。对于解码端的t时刻，根据源端的隐状态序列，和解码端前一时刻的输出、隐状态，以及该词在源端是否出现以及出现的位置，指导当前时刻产生每个词的可能性。由于我们将答案放在源端，问题放在了目标端，因此我们可以计算出每个候选答案能生成该问题的可能性。由此计算误差函数，进行训练。这样说来，我们是将seq2seq的生成模型变成了一个概率模型。

还有一点需要说明的是，将问题和答案反过来可以去除一些噪音，因为我们要生成的目标端序列是一致的，因此目标端t时刻之前的时刻不会因为序列不同对后一个词产生影响。

我们采用了支持pytorch语言的pytorch框架编程。
